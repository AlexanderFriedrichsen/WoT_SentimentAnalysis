{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgxEfSj4dYJU"
   },
   "source": [
    "Using the main text you chose at the start of the seasomester, plot happiness time series in the following ways using the labMT lexicon\n",
    "\n",
    "\n",
    "(a) Process (destroy) your text so that it is a simple text file with one 1-gram\n",
    "per line—a vector of 1-grams.\n",
    "To the extent possible, keep punctuation in as separate 1-grams. Periods,\n",
    "commas, semicolons, em dashes, ellipses, ldots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0Y8qmYEdYNx"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lr_5L2qafuE3",
    "outputId": "fc076f7b-cdc9-4de0-b2f1-4feccee977c4"
   },
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "# !pip install numpy\n",
    "# !pip install shifterator\n",
    "# !pip install nltk\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import shifterator\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9O_cfVMS0ZRl",
    "outputId": "3ad2298d-a0c4-4a35-db6a-2a3ef98ae6ed"
   },
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "oHjFXJM5gL4x"
   },
   "outputs": [],
   "source": [
    "#take all the .txt files for WoT and compile them into one\n",
    "import os\n",
    "rootdir = \"C:/Users/alexp/Documents/GitHub/WoT_SentimentAnalysis/txt\"\n",
    "\n",
    "lines = []\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #print(file)\n",
    "        with open(os.path.join(subdir, file), encoding=\"utf8\") as f:\n",
    "            lines.append(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCwwgQp8gL7t",
    "outputId": "756a8f8d-9303-4f47-bb89-b9a8c9d20a6a"
   },
   "outputs": [],
   "source": [
    "\n",
    "#ft = fulltext\n",
    "ft=\"\"\n",
    "for line in lines:\n",
    "    \n",
    "    ft += str(line)\n",
    "\n",
    "#how fast is this?\n",
    "nltk.word_tokenize(ft)\n",
    "\n",
    "\n",
    "#print(len(ft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay here need to put into a frequency dict of word:freq\n",
    "\n",
    "for word in ft:\n",
    "    cou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ft[1000:10000])\n",
    "\n",
    "\n",
    "hedonometer = pd.read_csv(\"/Hedonometer.csv\")\n",
    "hedonometer = hedonometer[[\"Word\", \"Happiness Score\"]]\n",
    "hedonometer\n",
    "\n",
    "\n",
    "\n",
    "function get_valence_dict()\n",
    "    val_dict = Dict()\n",
    "    happiness_str = load_file(pwd()*\"/hedonometer.csv\")\n",
    "\n",
    "    for row in split(happiness_str, '\\n')[2:end]\n",
    "        row_data = split(row, ',')\n",
    "        val_dict[String(strip(row_data[2], '\"'))] = parse(Float64, strip(row_data[4], '\"'))\n",
    "    end\n",
    "    # words are keys\n",
    "    return val_dict\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQnO922ddZia"
   },
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1=type2freq_1,\n",
    "                                      type2freq_1=type2freq_2,\n",
    "                                      type2score_1='labMT_English',\n",
    "                                      stop_lens=[(4,6)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltYTDKFJdZlo"
   },
   "outputs": [],
   "source": [
    "sentiment_shift = sh.WeightedAvgShift(type2freq_1,\n",
    "                                      type2freq_2,\n",
    "                                      'labMT_English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function get_valence_dict()\n",
    "    val_dict = Dict()\n",
    "    happiness_str = load_file(pwd()*\"/hedonometer.csv\")\n",
    "\n",
    "    for row in split(happiness_str, '\\n')[2:end]\n",
    "        row_data = split(row, ',')\n",
    "        val_dict[String(strip(row_data[2], '\"'))] = parse(Float64, strip(row_data[4], '\"'))\n",
    "    end\n",
    "    # words are keys\n",
    "    return val_dict\n",
    "end\n",
    "\n",
    "function get_average_valence(word_valence, gram_vector, happiness_words)\n",
    "    scored_words = gram_vector[in(happiness_words).(gram_vector)]\n",
    "    scores = getindex.(Ref(word_valence), scored_words)\n",
    "    if isempty(scores)\n",
    "        return nothing\n",
    "    end\n",
    "    return sum(scores) / length(scores)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "sizes = [1, 1.5, 2, 2.5, 3, 3.5, 4]\n",
    "\n",
    "# sizes = [1, 1.5, 2]\n",
    "println(\"Loading 1 grams...\")\n",
    "@time one_g = load_one_grams()\n",
    "println(\"Loading happiness data...\")\n",
    "@time word_valence = get_valence_dict()\n",
    "doc_length = length(one_g)\n",
    "happiness_words = keys(word_valence)\n",
    "h_avg = get_average_valence(word_valence, one_g, happiness_words)\n",
    "\n",
    "println(\"Calculating average over window...\")\n",
    "plot()\n",
    "@time for z in sizes\n",
    "    println(size, \" \", 10^z)\n",
    "    avgs = []\n",
    "    @time for i=1:doc_length\n",
    "        if i+round(Int,10^z) <= doc_length\n",
    "            avg = get_average_valence(word_valence, one_g[i:i+round(Int,10^z)], happiness_words)\n",
    "        else\n",
    "            avg = get_average_valence(word_valence, one_g[i:end], happiness_words)\n",
    "        end\n",
    "\n",
    "        if isnothing(avg)\n",
    "            append!(avgs, h_avg)\n",
    "        else\n",
    "            append!(avgs, avg)\n",
    "        end\n",
    "    end\n",
    "    plot!(avgs, labels=\"10^\"*string(z), legend=:bottomleft)\n",
    "    println(\"Done!\")\n",
    "    \n",
    "    println(\"saving plot...\")\n",
    "xlabel!(\"Index of first value in window\")\n",
    "ylabel!(\"average valence of window\")\n",
    "savefig(pwd()*\"/question_b.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nc_6mdkjfMpQ"
   },
   "source": [
    "Citations:\n",
    "\n",
    "Dodds, Peter Sheridan, Kameron Decker Harris, Isabel M. Kloumann, Catherine A. Bliss, and Christopher M. Danforth. “Temporal patterns of happiness and information in a global social network: Hedonometrics and Twitter.” PLoS ONE 6, no. 12 (2011)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "WoT_Sentiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
